{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education EDA\n",
    "This notebook explores educational assessment data provided from NCES. The original datasets were achievement broken down by subject, Local Educational Agency (LEA), and school year. For now, we'll focus on 2017-18.\n",
    "\n",
    "1. [Import Data](#Import-Data)\n",
    "\n",
    "2. [Preview Data and Structure](#Preview-Data)\n",
    "\n",
    "3.  [MATH ASSESSMENT](#Math-Assessment-Data)\n",
    "\n",
    "4. [Reading & Language Arts (RLA) Assessment](#Reading-and-Language-Arts-Assessment)\n",
    "\n",
    "[Census Broadband Dataa](#Census-Broadband-Data)\n",
    "\n",
    "[Census Device Data](#Census-Device-Data)\n",
    "\n",
    "[Geographical Data](#Geographical-Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to call data from website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "[Math Achievement Percentage Proficiency by Local Education Agency, 2017-18](https://www2.ed.gov/about/inits/ed/edfacts/data-files/math-achievement-lea-sy2017-18.csv)\n",
    "\n",
    "[Reading / Language Arts Achievement Percentage Proficiency by Local Education Agency, 2017-18](https://www2.ed.gov/about/inits/ed/edfacts/data-files/rla-achievement-lea-sy2017-18.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import 2018 LEA  math data to start\n",
    "\n",
    "# subject=input(\"Enter Assessment Subject: 'math' for math OR 'rla' for reading / language arts\")\n",
    "# geog=input(\"Enter Detail Level: 'lea' for Local Education Agency OR 'sch' for school\")\n",
    "# data_file=input(\"Enter dataset type: 'achievement' for assessment proficiency OR 'participation' for assessment participation OR 'acgr' for Adjusted Cohort Graduation Rate\")\n",
    "# school_year=input(\"Enter academic year as: YYYY-YY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='math'\n",
    "data_file='achievement'\n",
    "geog='lea'\n",
    "school_year='2017-18'\n",
    "\n",
    "math_2018 = pd.read_csv('https://www2.ed.gov/about/inits/ed/edfacts/data-files/{subject}-{data_file}-{geog}-sy{school_year}.csv'\n",
    "                 .format(subject=subject, data_file=data_file,  geog=geog, school_year=school_year),\n",
    "                 low_memory=False)\n",
    "math_2018.rename(columns={'LEAID':'leaid'}, inplace=True)\n",
    "math_2018['leaid'] = math_2018['leaid'].astype(str)\n",
    "math_2018['leaid'] = math_2018['leaid'].str.lstrip('9700000US').str.zfill(7)\n",
    "math_2018.to_pickle('../data/{subject}_{data_file}_{school_year}.pkl'.format(subject=subject, data_file=data_file, school_year=school_year))\n",
    "math_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also Import RLA 2018 data\n",
    "\n",
    "# subject=input(\"Enter Assessment Subject: 'math' for math OR 'rla' for reading / language arts\")\n",
    "# geog=input(\"Enter Detail Level: 'lea' for Local Education Agency OR 'sch' for school\")\n",
    "# data_file=input(\"Enter dataset type: 'achievement' for assessment proficiency OR 'participation' for assessment participation OR 'acgr' for Adjusted Cohort Graduation Rate\")\n",
    "# school_year=input(\"Enter academic year as: YYYY-YY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='rla'\n",
    "data_file='achievement'\n",
    "geog='lea'\n",
    "school_year='2017-18'\n",
    "\n",
    "rla_2018 = pd.read_csv('https://www2.ed.gov/about/inits/ed/edfacts/data-files/{subject}-{data_file}-{geog}-sy{school_year}.csv'\n",
    "                 .format(subject=subject, data_file=data_file,  geog=geog, school_year=school_year),\n",
    "                 low_memory=False)\n",
    "rla_2018.rename(columns={'LEAID':'leaid'}, inplace=True)\n",
    "rla_2018['leaid'] = rla_2018['leaid'].astype(str)\n",
    "rla_2018['leaid'] = rla_2018['leaid'].str.lstrip('9700000US').str.zfill(7)\n",
    "rla_2018.to_pickle('../data/{subject}_{data_file}_{school_year}.pkl'.format(subject=subject, data_file=data_file, school_year=school_year))\n",
    "rla_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_2018 = pd.merge(math_2018, rla_2018, how='inner', on='leaid')\n",
    "assessment_2018.to_pickle('../data/assessment_2018.pkl')\n",
    "assessment_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2018 = pd.read_pickle('ACS_5yr_DP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_2018.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows\n",
    "There are almost 16,000 rows here. They appear to be unique observations for each Local Education Agency (LEA), but let's confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure that LEAID is a unique value for each observation\n",
    "math_2018['LEAID'].nunique() == math_2018.shape[0]\n",
    "## Returns True if all values are unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Since the unique number of LEAIDs (15984) matches the total number of observations for the dataset, we know that it is a unique value. We can use this value as our index later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns\n",
    "To preview what's in them better, we'll look at just the columns to see if we can eliminate redundancies/noise.\n",
    "\n",
    "The first set of columns contains identifying data:\n",
    "- STNAME - State Name\n",
    "- FIPST - ANSI 2-digit code for state\n",
    "- LEAID - NCES-assigned District ID\n",
    "- ST_LEAID - State-assigned District ID\n",
    "- LEANM - District Name\n",
    "- DATE_CUR - Date of data snapshot\n",
    "\n",
    "And the rest contain various groupings of the data for each observation (LEA) in the following format:\n",
    "- \\[SUBGROUP\\]_\\[SUBJECT\\]\\[GRADE\\]\\[METRIC\\]_\\[SCHOOLYEAR\\]\n",
    "\n",
    "Reading the [documentation]('..\\data\\education\\education_documentation_2018.docx) provides an overview of those columns, along with their corresponding meanings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Subgroups** - Major Racial and Ethnic Groups / Special Populations\n",
    "    - ALL - All Students in the school\n",
    "    - Racial Subgroups (begin with M):\n",
    "        - MAM American Indian/Alaska Native\n",
    "        - MAS Asian/Pacific Islander\n",
    "        - MHI Hispanic\n",
    "        - MBL Black\n",
    "        - MWH White\n",
    "        - MTR Two or More Races\n",
    "    - Special Population Subgroups:\n",
    "        - CWD Children with disabilities\n",
    "        - ECD Economically disadvantaged\n",
    "        - LEP Limited English proficiency\n",
    "        - HOM Homeless\n",
    "        - MIG Migrant\n",
    "        - FCS Foster Care Status\n",
    "        - MIL Military Connected (new for '17-'18)\n",
    "        \n",
    "- **Subject** - Specific to each file.\n",
    "    - Math (MTH) or Reading/Language Arts (RLA)\n",
    "    \n",
    "- **Grade** \n",
    "    - 00 - aggregated across all grades\n",
    "    - 03-08 - Grades 3-8\n",
    "    - HS - High School\n",
    "    \n",
    "- **Metric**\n",
    "    - numvalid - # of students who completed assessment proficient\n",
    "    - pctprof - % of students proficient or higher\n",
    "    \n",
    "- **School Year**\n",
    "    - Limited to 2017-18 in this case\n",
    "    - May be able to add additional years later for better understanding\n",
    "    \n",
    "So, for example, the column labeled \"**MBL_MTH08PCTPROF_1718**\" describes: \n",
    "- in 2017-18 **\\[1718\\]**,\n",
    "- for Black students **\\[MBL\\]** \n",
    "- studying Math **\\[MTH\\]**\n",
    "- in the Eighth grade **\\[08\\]**, \n",
    "- what Percentage were deemed Proficient **\\[PCTPROF\\]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing and observing this breakdown might prove helpful in improving the model later. For now, however, we'll keep only the following columns:\n",
    "- LEAID - as our unique identifier / index. We'll convert this to a 7-digit string for concatenation.\n",
    "- ALL_MTHHSPCTPROF_1718 - as our prediction column.\n",
    "\n",
    "Given the business understanding and limitation to HS students, this makes sense. Since we don't have a total number of students who took the exam, percentage proficient will standardize the number proficient across districts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Assessment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a\n",
    "    \n",
    "#     elif \"GT\" or \"GE\" in score:\n",
    "#         if \"GT\" in score:\n",
    "#             score = score.strip(\"GT\")\n",
    "#             score = (100 - int(score))/2\n",
    "#             return round(int((score)))\n",
    "\n",
    "#         elif \"GE\" in score:\n",
    "#             score = score.strip(\"GE\")\n",
    "#             score = (100 - int(score))/2\n",
    "#             return round(int((score)))\n",
    "    \n",
    "#     elif \"LT\" or \"LE\" in score:\n",
    "#         if \"LT\" in score:\n",
    "#             score = score.strip(\"LT\")\n",
    "#             score = (0 + int(score))/2\n",
    "#             return round(int((score)))\n",
    "\n",
    "#         elif \"LE\" in score:\n",
    "#             score = score.strip(\"LE\")\n",
    "#             score = (0 + int(score))/2\n",
    "#             return round(int((score)))\n",
    "    else:\n",
    "        return int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new dataframe with just the two desired columns\n",
    "all_math_2018 = math_2018[['LEAID','ALL_MTHHSPCTPROF_1718']].dropna()\n",
    "all_math_2018.columns = all_math_2018.columns.str.lower()\n",
    "all_math_2018.rename(columns={'all_mthhspctprof_1718': 'math_score'}, inplace=True)\n",
    "all_math_2018.leaid = all_math_2018.leaid.apply(str)\n",
    "all_math_2018.math_score = all_math_2018.math_score.apply(str)\n",
    "all_math_2018.leaid = all_math_2018.leaid.str.zfill(7)\n",
    "all_math_2018 = pd.DataFrame(data=all_math_2018)\n",
    "print(all_math_2018.head())\n",
    "print(all_math_2018.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_values = all_math_2018[all_math_2018['math_score'] == \"PS\" ].index\n",
    "period_values = all_math_2018[all_math_2018['math_score'] == \".\" ].index\n",
    "LE_values = all_math_2018[all_math_2018.math_score.str.contains(\"LE\")].index\n",
    "GT_values = all_math_2018[all_math_2018.math_score.str.contains(\"GE\")].index\n",
    "GE_values = all_math_2018[all_math_2018.math_score.str.contains(\"GT\")].index\n",
    "LT_values = all_math_2018[all_math_2018.math_score.str.contains(\"LT\")].index\n",
    "\n",
    "all_math_2018.drop(PS_values, inplace = True)\n",
    "all_math_2018.drop(period_values, inplace = True)\n",
    "all_math_2018.drop(LT_values, inplace = True)\n",
    "all_math_2018.drop(LE_values, inplace = True)\n",
    "all_math_2018.drop(GT_values, inplace = True)\n",
    "all_math_2018.drop(GE_values, inplace = True)\n",
    "all_math_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_math_2018.math_score = all_math_2018.math_score.apply(score_cleaner)\n",
    "all_math_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_math_2018.math_score = all_math_2018.loc[~all_math_2018.math_score.str.contains(\"LT\"),:]\n",
    "# all_math_2018.dropna(inplace=True)\n",
    "# all_math_2018.math_score = all_math_2018.loc[~all_math_2018.math_score.str.contains(\"GE\"),:]\n",
    "# all_math_2018.dropna(inplace=True)\n",
    "# all_math_2018.math_score = all_math_2018.loc[~all_math_2018.math_score.str.contains(\"GT\"),:]\n",
    "# all_math_2018.dropna(inplace=True)\n",
    "\n",
    "# all_math_2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is much cleaner. \n",
    "\n",
    "However, looking closer at the target variable column we have two issues in the target variable column:\n",
    "1. Null values - around 3600\n",
    "2. Datatype - is classified as object\n",
    "\n",
    "Not only is the target variable classified as an object, but looking at the 5th observation, the objects do not appear to be readily convertible to integers, since some of them appear to be ranges. Taking a further look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_math_2018['math_score'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further inspection reveals that \"nan\" values, \".\" values (also described as blank in documentation),  ranges, and strings are complicating the numerical analysis. Referring again to the documentation, these variations were intentionally introduced for student privacy concerns, with the following codes:\n",
    "\n",
    "- GE - Greater than or equal to\n",
    "- LE - Less than or equal to\n",
    "- GT Greater than\n",
    "- LT - Less than\n",
    "- PS - Privacy Suppressed (<5 students)\n",
    "\n",
    "Specifically, as specified in Part 1.5 and Table 2 of the documentation for 2017-18, for schools with less than 300 students, \"As the number of students reported increases, the magnitude of the range decreases\". \n",
    "\n",
    "Given these ranges, we can make a few choices:\n",
    "\n",
    "1. Limit the study to large high schools (300+). \n",
    "    - Benefit: Leaves only whole numbers, enabling integer conversion. \n",
    "    - Benefit: Allows for the possible usage of linear regression since it would be a continuous variable\n",
    "    - Cost: Eliminates the potential for analyzing size as a factor; Unnecessarily eliminates data\n",
    "    \n",
    "\n",
    "2. Bin the score ranges...more options here: \n",
    "    - Smaller scale (e.g., 25-29)\n",
    "    - Medium-scale (e.g., Very High proficiency (>80%), high proficiency (60-80), proficient (40-60), low proficiency (20-40), very low proficiency (0-20)\n",
    "    - Large scale (e.g., >= 50% as proficient)\n",
    "    \n",
    "\n",
    "For now, let's focus on large high schools. In this dataset, this means limiting the observations selected to those with 1 or 2 characters that are not \"PS\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_math_2018.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using only the schools that hadn't been \"blurred\" will result in too few datapoints. For now, though, let's continue by repeating this procedure for Reading and Language Arts data (RLA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Language Arts Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rla_2018 = pd.read_csv('../data/education/rla-achievement-lea-sy2017-18.csv', low_memory=False)\n",
    "\n",
    "## Create new dataframe with just the two desired columns\n",
    "all_rla_2018 = rla_2018[['LEAID','ALL_RLAHSPCTPROF_1718']].dropna()\n",
    "all_rla_2018.columns = all_rla_2018.columns.str.lower()\n",
    "all_rla_2018.rename(columns={'all_rlahspctprof_1718': 'rla_score'}, inplace=True)\n",
    "all_rla_2018.leaid = all_rla_2018.leaid.apply(str)\n",
    "all_rla_2018.rla_score = all_rla_2018.rla_score.apply(str)\n",
    "all_rla_2018.leaid = all_rla_2018.leaid.str.zfill(7)\n",
    "all_rla_2018 = pd.DataFrame(data=all_rla_2018)\n",
    "print(all_rla_2018.head())\n",
    "print(all_rla_2018.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rla_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_values = all_rla_2018[all_rla_2018['rla_score'] == \"PS\" ].index\n",
    "period_values = all_rla_2018[all_rla_2018['rla_score'] == \".\" ].index\n",
    "LE_values = all_rla_2018[all_rla_2018.rla_score.str.contains(\"LE\")].index\n",
    "GT_values = all_rla_2018[all_rla_2018.rla_score.str.contains(\"GE\")].index\n",
    "GE_values = all_rla_2018[all_rla_2018.rla_score.str.contains(\"GT\")].index\n",
    "LT_values = all_rla_2018[all_rla_2018.rla_score.str.contains(\"LT\")].index\n",
    "\n",
    "all_rla_2018.drop(PS_values, inplace = True)\n",
    "all_rla_2018.drop(period_values, inplace = True)\n",
    "all_rla_2018.drop(LT_values, inplace = True)\n",
    "all_rla_2018.drop(LE_values, inplace = True)\n",
    "all_rla_2018.drop(GT_values, inplace = True)\n",
    "all_rla_2018.drop(GE_values, inplace = True)\n",
    "all_rla_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rla_2018.rla_score = all_rla_2018.rla_score.apply(score_cleaner)\n",
    "all_rla_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rla_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_assessments_2018 = pd.merge(all_rla_2018, all_math_2018, how='inner', on='leaid').dropna()\n",
    "hs_assessments_2018['year'] = 2018\n",
    "hs_assessments_2018.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_assessments_2018.to_pickle('../data/education/hs_assessments_2018.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(assessments_2018, vars=['math_score', 'rla_score'], kind='reg', diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is probably expected, there is a definite correlation between Reading/Language Arts Scores and Math Scores for the limited dataset we're observing. It should be noted that the math score distribution skews slightly to the left. The RLA score skews to the right. This phenomenon is confirmed in the statistical summary which shows us that the mean math score is 45% and the mean reading/language arts score is 55%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-12 math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_math_2018 = math_2018[['LEAID','ALL_MTH00PCTPROF_1718']].dropna()\n",
    "whole_math_2018.columns = whole_math_2018.columns.str.lower()\n",
    "whole_math_2018.rename(columns={'all_mth00pctprof_1718': 'math_score'}, inplace=True)\n",
    "whole_math_2018.leaid = whole_math_2018.leaid.apply(str)\n",
    "whole_math_2018.math_score = whole_math_2018.math_score.apply(str)\n",
    "whole_math_2018.leaid = whole_math_2018.leaid.str.zfill(7)\n",
    "whole_math_2018 = pd.DataFrame(data=whole_math_2018)\n",
    "print(whole_math_2018.head())\n",
    "print(whole_math_2018.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_values = whole_math_2018[whole_math_2018['math_score'] == \"PS\" ].index\n",
    "period_values = whole_math_2018[whole_math_2018['math_score'] == \".\" ].index\n",
    "LE_values = whole_math_2018[whole_math_2018.math_score.str.contains(\"LE\")].index\n",
    "GT_values = whole_math_2018[whole_math_2018.math_score.str.contains(\"GE\")].index\n",
    "GE_values = whole_math_2018[whole_math_2018.math_score.str.contains(\"GT\")].index\n",
    "LT_values = whole_math_2018[whole_math_2018.math_score.str.contains(\"LT\")].index\n",
    "\n",
    "whole_math_2018.drop(PS_values, inplace = True)\n",
    "whole_math_2018.drop(period_values, inplace = True)\n",
    "whole_math_2018.drop(LT_values, inplace = True)\n",
    "whole_math_2018.drop(LE_values, inplace = True)\n",
    "whole_math_2018.drop(GT_values, inplace = True)\n",
    "whole_math_2018.drop(GE_values, inplace = True)\n",
    "whole_math_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_math_2018.math_score = whole_math_2018.math_score.apply(score_cleaner)\n",
    "whole_math_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_math_2018.to_pickle('../data/education/whole_math_2018.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-12 rla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new dataframe with just the two desired columns\n",
    "\n",
    "whole_rla_2018 = rla_2018[['LEAID','ALL_RLA00PCTPROF_1718']].dropna()\n",
    "whole_rla_2018.columns = whole_rla_2018.columns.str.lower()\n",
    "whole_rla_2018.rename(columns={'all_rla00pctprof_1718': 'rla_score'}, inplace=True)\n",
    "whole_rla_2018.leaid = whole_rla_2018.leaid.apply(str)\n",
    "whole_rla_2018.rla_score = whole_rla_2018.rla_score.apply(str)\n",
    "whole_rla_2018.leaid = whole_rla_2018.leaid.str.zfill(7)\n",
    "whole_rla_2018 = pd.DataFrame(data=whole_rla_2018)\n",
    "print(whole_rla_2018.head())\n",
    "print(whole_rla_2018.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_values = whole_rla_2018[whole_rla_2018['rla_score'] == \"PS\" ].index\n",
    "period_values = whole_rla_2018[whole_rla_2018['rla_score'] == \".\" ].index\n",
    "LE_values = whole_rla_2018[whole_rla_2018.rla_score.str.contains(\"LE\")].index\n",
    "GT_values = whole_rla_2018[whole_rla_2018.rla_score.str.contains(\"GE\")].index\n",
    "GE_values = whole_rla_2018[whole_rla_2018.rla_score.str.contains(\"GT\")].index\n",
    "LT_values = whole_rla_2018[whole_rla_2018.rla_score.str.contains(\"LT\")].index\n",
    "\n",
    "whole_rla_2018.drop(PS_values, inplace = True)\n",
    "whole_rla_2018.drop(period_values, inplace = True)\n",
    "whole_rla_2018.drop(LT_values, inplace = True)\n",
    "whole_rla_2018.drop(LE_values, inplace = True)\n",
    "whole_rla_2018.drop(GT_values, inplace = True)\n",
    "whole_rla_2018.drop(GE_values, inplace = True)\n",
    "whole_rla_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_rla_2018.rla_score = whole_rla_2018.rla_score.apply(score_cleaner)\n",
    "whole_rla_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_rla_2018.to_pickle('../data/education/whole_rla_2018.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-12 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_assessments_2018 = pd.merge(whole_rla_2018, whole_math_2018, how='inner', on='leaid').dropna()\n",
    "combined_assessments_2018['year'] = 2018\n",
    "combined_assessments_2018.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_assessments_2018.to_pickle('../data/education/combined_assessments_2018.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Broadband Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Device Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5_2018 = pd.read_pickle('../data/digital/acs5_2018.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_working_set = pd.read_pickle('../data/full_working_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.read_pickle('../data/complete_df.pkl')\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
